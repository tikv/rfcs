# Batch Client Messages

## Summary

This proposal adds a new API `BatchCommands` in `Tikv` service, which
contains a vector of raw client requests (e.g. `GetRequest`). It can reduce
gRPC messages count dramatically; and, according to tests, the performance
is improved considerably, especially when TiKV or TiDB's CPU usage reaches
the bottleneck.

## Motivation

In the current implementation, the default configuration of `grpc-concurrency`
is `4`, which could be a bottleneck for TiKV under heavy load. Although we can
increase the configuration value, or make it adaptable, we still need
to try the best to reduce the gRPC CPU usage. With the `BatchCommands` interface,
clients can consolidate requests in a batch, and send them to TiKV with
less `send` (or `sendmsg`) syscalls, which normally means less gRPC CPU usage.

## Detailed design

### BatchCommands interface

The new interface `BatchCommands` in `Tikv` service looks like this:

```proto
service Tikv {
    // ... some old interfaces.
    rpc BatchCommands(stream BatchCommandsRequest) returns (stream BatchCommandsResponse) {}
}

message BatchCommandsRequest {
    repeated Request requests = 1;
    repeated uint64 request_ids = 2;

    message Request {
        oneof cmd {
            GetRequest Get = 1;
            ScanRequest Scan = 2;
            // ... more other requests.
        }
    }
}

message BatchCommandsResponse {
    repeated Response responses = 1;
    repeated uint64 request_ids = 2;
    // Indicates whether the TiKV's transport layer is under heavy load or not.
    uint64 transport_layer_load = 3;

    message Response {
        oneof cmd {
            GetResponse Get = 1;
            ScanResponse Scan = 2;
            // ... more other responses respectively.
        }
    }
}
```

`BatchCommands` is a dual-way streaming interface. This means TiKV will constuct
responses for batched requests. For example, TiDB can send a
`BatchCommandsRequest` with requests `[Req-1, Req-2, Req-3, Req-4, Req-5]`.
However, TiKV may send back `BatchCommandsResponse`s with responses `[Resp-1,
Resp-5, Resp-3]` and `[Resp-2, Resp-4]`. It's unnecessary to wait for any
single response that comes from one `BatchCommandsRequest`; the order depends
on processing speed of each request.

`request_ids` in both `BatchCommandsRequest` and `BatchCommandsResponse` is
used to map requests and responses in the streaming interface so that TiDB can
know about the correlation.

We use `oneof` instead of `message` to unify requests and responses. Their
wired protocols are almost the same, but the former generates much better code.

`transport_layer_load` in `BatchCommandsResponse` is
used by TiKV to tell clients its current load, so clients can adjust
their strategy (e.g. add some backoff time to avoid overly small batch) to be more
efficient for TiKV.

### Implementation in TiKV

The implementation in TiKV is very simple. TiKV just extracts `Request`s from
`BatchCommandsRequest`, dispatches them to the engine layer, consolidates the
`Response`s into `BatchCommandsResponse`, and sends it to clients.

### Implement in TiDB

The implementation in TiDB is a little complex. One TiDB server can establish many
connections to one TiKV server, and for every connection TiDB will construct a map
to associate requests with some IDs, which are generated by TiDB to trace requests.
Before sending requests to TiKV, TiDB needs to decide whether to wait for a while
to collect a bigger batch or not. In our design, this depends on TiKVs' load returned
in `BatchCommandsResponse`. If the load is greater than a configurable threshold,
TiDB will wait for 2 milliseconds before sending the requests.

### How TiKV knows its gRPC threads are busy

TiKV gets CPU usage of gRPC threads by reading
`/proc/<tikv-pid>/tasks/<grpc-tid>`, which is only avaliable on Linux. If gRPC
CPU usage is greater than 80% in last second, TiKV can inform clients that it's
overloaded.

However, this function is not available on other operating systems, in which
case TiKV will never tell clients it's overloaded, which means
`transport_layer_load` will be 0.

## Drawbacks

As you can see the *wait algorithm* we introduce in TiDB will surely cause a
higher latency when the algorithm works. However, a bigger batch means less
syscalls, and then less cost on network. So we must choose the *threshold*
carefully to avoid potential performance drops.
